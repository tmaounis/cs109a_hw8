{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science: \n",
    "\n",
    "## Homework 8: Ensembles: Bagging, Random Forests, and Boosting\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2018**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the [instructions given in Canvas](https://canvas.harvard.edu/courses/42693/pages/homework-policies-and-submission-instructions).\n",
    "\n",
    "- If needed, clarifications will be posted on Piazza.\n",
    "\n",
    "- This homework can be submitted in pairs.\n",
    "\n",
    "- If you submit individually but you have worked with someone, please include the name of your **one** partner below. \n",
    "\n",
    "\n",
    "**Name of the person you have worked with goes here:**\n",
    "<br><BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "Completing this assignment will demonstrate success at the following objectives:\n",
    "\n",
    "- Statistical\n",
    "  - Predict when bagging will help model performance.\n",
    "  - Identify how Random Forests improve over bagging.\n",
    "  - Predict when boosting will help model performance.\n",
    "  - Compare and contrast bagging and boosting.\n",
    "- Coding\n",
    "  - Identify and fix problems in poorly written code\n",
    "- Communication\n",
    "  - Visually explain a complex concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"theme\"> Overview: Higgs Boson Discovery </div>\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. We shall explore the use of ensemble methods for this classification task.\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 training samples, 5000 test samples\n",
      "\n",
      "Columns:\n",
      "lepton pT, lepton eta, lepton phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb, class\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_train = pd.read_csv('data/Higgs_train.csv')\n",
    "data_test = pd.read_csv('data/Higgs_test.csv')\n",
    "\n",
    "print(f\"{len(data_train)} training samples, {len(data_test)} test samples\")\n",
    "print(\"\\nColumns:\")\n",
    "print(', '.join(data_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.377</td>\n",
       "      <td>-1.5800</td>\n",
       "      <td>-1.7100</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.114</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.280</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>1.170</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.886</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.7620</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.160</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>-1.3500</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-0.0194</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851</td>\n",
       "      <td>-0.3810</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.620</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.796</td>\n",
       "      <td>-1.520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768</td>\n",
       "      <td>-0.6920</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>1.320</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT  lepton eta  lepton phi  missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  jet 2 eta  jet 2 phi  jet 2 b-tag  jet 3 pt  jet 3 eta  jet 3 phi  jet 3 b-tag  jet 4 pt  jet 4 eta  jet 4 phi  jet 4 b-tag   m_jj  m_jjj   m_lv  m_jlv   m_bb  m_wbb  m_wwbb  class\n",
       "0      0.377     -1.5800     -1.7100                     0.991               0.114     1.250      0.620     -1.480         2.17     0.754     0.7750     -0.667         2.21     1.280     -1.190      0.505         0.00     1.110     -0.464      0.397         0.00  0.522  1.320  0.982  1.360  0.965  1.310   1.080    1.0\n",
       "1      0.707      0.0876     -0.4000                     0.919              -1.230     1.170     -0.553      0.886         2.17     1.300     0.7620     -1.060         2.21     0.607      0.459      1.020         0.00     0.497      0.956      0.236         0.00  0.440  0.829  0.992  1.160  2.220  1.190   0.938    1.0\n",
       "2      0.617      0.2660     -1.3500                     1.150               1.040     0.955      0.377     -0.148         0.00     1.060    -0.0194      1.110         0.00     1.470      0.205     -1.060         2.55     1.490     -0.398     -0.542         0.00  1.020  1.030  0.986  0.928  1.370  0.982   0.917    1.0\n",
       "3      0.851     -0.3810     -0.0713                     1.470              -0.795     0.692      0.883      0.497         0.00     1.620     0.1240      1.180         1.11     1.290      0.160     -0.916         2.55     0.945      0.796     -1.520         0.00  1.200  1.100  0.987  1.350  1.460  0.995   0.954    1.0\n",
       "4      0.768     -0.6920     -0.0402                     0.615               0.144     0.749      0.397     -0.874         0.00     1.150     0.1270      1.320         2.21     0.730     -0.758     -1.120         0.00     0.848      0.107      0.502         1.55  0.922  0.864  0.983  1.370  0.601  0.919   0.957    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.978645</td>\n",
       "      <td>-0.014280</td>\n",
       "      <td>-0.018956</td>\n",
       "      <td>1.005793</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.980390</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>0.993678</td>\n",
       "      <td>0.988659</td>\n",
       "      <td>-0.010310</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>1.006922</td>\n",
       "      <td>0.997004</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>1.011994</td>\n",
       "      <td>0.982806</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>1.007810</td>\n",
       "      <td>1.038431</td>\n",
       "      <td>1.027201</td>\n",
       "      <td>1.054719</td>\n",
       "      <td>1.023094</td>\n",
       "      <td>0.958464</td>\n",
       "      <td>1.033432</td>\n",
       "      <td>0.960494</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.547025</td>\n",
       "      <td>1.011927</td>\n",
       "      <td>0.997945</td>\n",
       "      <td>0.591907</td>\n",
       "      <td>1.003337</td>\n",
       "      <td>0.463677</td>\n",
       "      <td>1.002018</td>\n",
       "      <td>1.014559</td>\n",
       "      <td>1.028920</td>\n",
       "      <td>0.476462</td>\n",
       "      <td>1.007983</td>\n",
       "      <td>1.002177</td>\n",
       "      <td>1.045206</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>1.007824</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.200416</td>\n",
       "      <td>0.497681</td>\n",
       "      <td>1.007999</td>\n",
       "      <td>1.008904</td>\n",
       "      <td>1.400846</td>\n",
       "      <td>0.619460</td>\n",
       "      <td>0.353984</td>\n",
       "      <td>0.173243</td>\n",
       "      <td>0.427141</td>\n",
       "      <td>0.495720</td>\n",
       "      <td>0.352966</td>\n",
       "      <td>0.306057</td>\n",
       "      <td>0.499444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>-2.410000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>-2.920000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>-2.720000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.587000</td>\n",
       "      <td>-0.764250</td>\n",
       "      <td>-0.877500</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>-0.659250</td>\n",
       "      <td>-0.885000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>-0.699000</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664750</td>\n",
       "      <td>-0.679250</td>\n",
       "      <td>-0.858000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>-0.707250</td>\n",
       "      <td>-0.869250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798750</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.772750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.846000</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>-0.023500</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.030700</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>-0.004700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.330000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>5.740000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>6.220000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton pT   lepton eta   lepton phi  missing energy magnitude  missing energy phi     jet 1 pt    jet 1 eta    jet 1 phi  jet 1 b-tag     jet 2 pt    jet 2 eta    jet 2 phi  jet 2 b-tag     jet 3 pt    jet 3 eta    jet 3 phi  jet 3 b-tag     jet 4 pt    jet 4 eta    jet 4 phi  jet 4 b-tag         m_jj        m_jjj         m_lv        m_jlv         m_bb        m_wbb       m_wwbb        class\n",
       "count  5000.000000  5000.000000  5000.000000               5000.000000         5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000\n",
       "mean      0.978645    -0.014280    -0.018956                  1.005793            0.002528     0.980390     0.025014    -0.007104     0.993678     0.988659    -0.010310    -0.006926     1.006922     0.997004     0.018817     0.003952     1.011994     0.982806     0.005201     0.003349     1.007810     1.038431     1.027201     1.054719     1.023094     0.958464     1.033432     0.960494     0.524600\n",
       "std       0.547025     1.011927     0.997945                  0.591907            1.003337     0.463677     1.002018     1.014559     1.028920     0.476462     1.007983     1.002177     1.045206     0.471681     1.007824     0.999656     1.200416     0.497681     1.007999     1.008904     1.400846     0.619460     0.353984     0.173243     0.427141     0.495720     0.352966     0.306057     0.499444\n",
       "min       0.275000    -2.410000    -1.740000                  0.010000           -1.740000     0.170000    -2.920000    -1.740000     0.000000     0.198000    -2.910000    -1.740000     0.000000     0.265000    -2.720000    -1.740000     0.000000     0.366000    -2.500000    -1.740000     0.000000     0.151000     0.443000     0.339000     0.371000     0.079500     0.413000     0.452000     0.000000\n",
       "25%       0.587000    -0.764250    -0.877500                  0.581000           -0.870000     0.676000    -0.659250    -0.885000     0.000000     0.666000    -0.699000    -0.859500     0.000000     0.664750    -0.679250    -0.858000     0.000000     0.619000    -0.707250    -0.869250     0.000000     0.798750     0.850000     0.986000     0.768000     0.672000     0.826000     0.772750     0.000000\n",
       "50%       0.846000    -0.009305    -0.016050                  0.903500            0.001300     0.891000     0.049500    -0.023500     1.090000     0.891000    -0.004800    -0.030700     1.110000     0.899500     0.045700     0.018800     0.000000     0.877000     0.012900    -0.004700     0.000000     0.898000     0.957000     0.990000     0.922000     0.868000     0.952000     0.877500     1.000000\n",
       "75%       1.220000     0.725500     0.837000                  1.300000            0.866000     1.160000     0.716000     0.894000     2.170000     1.192500     0.692250     0.855500     2.210000     1.232500     0.717000     0.855000     2.550000     1.220000     0.719000     0.859000     3.100000     1.030000     1.090000     1.030000     1.160000     1.120000     1.140000     1.060000     1.000000\n",
       "max       5.330000     2.430000     1.740000                  6.260000            1.740000     4.190000     2.960000     1.740000     2.170000     4.800000     2.910000     1.740000     2.210000     4.630000     2.730000     1.740000     2.550000     5.770000     2.490000     1.740000     3.100000    10.600000     5.740000     3.940000     6.220000     5.080000     4.320000     3.500000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(data_train.head())\n",
    "display(data_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into NumPy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != 'class'].values\n",
    "y_train = data_train['class'].values\n",
    "X_test = data_test.iloc[:, data_test.columns != 'class'].values\n",
    "y_test = data_test['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"> <b> Question 1: A Single Model [20 pts]</b> </div>\n",
    "\n",
    "We start by fitting a basic model we can compare the other models to. We'll pick a decision tree as the base model, because we'll later include random forests and want a fair comparison. We'll tune the decision tree using cross-validation. As usual, we'll be tuning the maximum tree depth; we refer to this parameter as \"depth\" for simplicity.\n",
    "\n",
    "Since we will only be using tree-based methods in this homework, we do not need to standardize or normalize the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1.1**: Fit a decision tree model to the training set. Choose a range of tree depths to evaluate. Plot the estimated performance +/- 2 standard deviations for each depth using 5-fold cross validation. Also include the training set performance in your plot, but set the y-axis to focus on the cross-validation performance.\n",
    "\n",
    "*Hint*: use `plt.fill_between` to shade the region.\n",
    "\n",
    "**1.2** Select an appropriate depth and justify your choice. Using your cross-validation estimates, report the mean +/- 2 stdev. Then report the classification accuracy on the test set. (Store the training and test accuracies in variables to refer to in a later question.)\n",
    "\n",
    "**1.3** What is the mechanism by which limiting the depth of the tree avoids over-fitting? What is one downside of limiting the tree depth? Your answer should refer to the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_DecisionTree(x_train, y_train, max_depth):\n",
    "    #Returns best model and score from training set cross validation\n",
    "    #also plots accuracy score\n",
    "    depths = range(1,max_depth+1)\n",
    "    cvmeans=[] #list of mean cross validation scores\n",
    "    cvstds=[]\n",
    "    scores_acc = [] #list of accuracy scores\n",
    "    best_score = 0 #best cross validation mean score\n",
    "    for i in depths:\n",
    "        model = DecisionTreeClassifier(max_depth=i, random_state=11).fit(x_train, y_train)\n",
    "        cv = cross_val_score(model, x_train, y_train, cv=5)\n",
    "        cvmean = cv.mean()\n",
    "        cvstd = cv.std()\n",
    "        cvmeans.append(cvmean)\n",
    "        cvstds.append(cvstd)\n",
    "        y_pred = model.predict(x_train)\n",
    "        score_acc = accuracy_score(y_train, y_pred)\n",
    "        scores_acc.append(score_acc)\n",
    "        if cvmean > best_score:\n",
    "            best_score=cvmean\n",
    "            best_model=model\n",
    "    cvmeans=np.array(cvmeans)\n",
    "    cvstds=np.array(cvstds)\n",
    "    plt.plot(depths, cvmeans, \"*-\", label=\"Mean Cross Validation Score\")\n",
    "    plt.fill_between(depths, cvmeans - 2*cvstds, cvmeans + 2*cvstds, alpha=0.3)\n",
    "    plt.plot(depths, scores_acc, \".-\", label=\"Accuracy Score\")\n",
    "    plt.title(\"Scores for Varying Depth Decision Trees\")\n",
    "    plt.xlabel(\"Depth\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(depths)\n",
    "    plt.ylim(.5,1)\n",
    "    plt.legend()\n",
    "    return best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_DT, best_score = get_best_DecisionTree(X_train, y_train, 12)\n",
    "#set the y-axis to focus on the cross-validation performance???????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best decision tree had depth 5 with a mean cross validation score of 0.64 (+- 0.02).\n"
     ]
    }
   ],
   "source": [
    "cv = cross_val_score(model_DT, X_train, y_train, cv=5)\n",
    "print(\"The best decision tree had depth %i with a mean cross validation score of %.2f (+- %.2f).\"\n",
    "      %(model_DT.max_depth,cv.mean(),2*cv.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the training set, the classification accuracy is 0.68.\n",
      "On the test set, the classification accuracy is 0.65.\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = model_DT.predict(X_test)\n",
    "y_pred_train = model_DT.predict(X_train)\n",
    "model_DT_train_acc = accuracy_score(y_train, y_pred_train)\n",
    "model_DT_test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(\"On the training set, the classification accuracy is %.2f.\"%model_DT_train_acc)\n",
    "print(\"On the test set, the classification accuracy is %.2f.\"%model_DT_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A depth 5 decision tree is appropriate. It achieves the best 5 fold mean cross validation score. Based on its training and test accuracies, we observe the model is not terrible but not great. Additionally, the test set accuracy is close to the training set accuracy, so it does not seem that the model is overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"> <b> Question 2: Bagging [25 pts]</b> </div>\n",
    "\n",
    "Bagging is the technique of building the same model on multiple bootstraps from the data and combining each model's prediction to get an overall classification. In this question we build an example by hand and study how the number of bootstrapped datasets impacts the accuracy of the resulting classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**2.1** Choose a tree depth that will overfit the training set. What evidence leads you to believe that this depth will overfit? Assign your choice to a variable here. (You may want to explore different settings for this value in the problems below.)\n",
    "\n",
    "**2.2** Create 45 bootstrapped replications of the original training data, and fit a decision tree to each. Use the tree depth you just chose in 2.1. Record each tree's prediction. In particular, produce a dataset like those below, where each row is a training (or test) example, each column is one of the trees, and each entry is that tree's prediction for that example. (Labeling the rows and columns is optional.)\n",
    "\n",
    "Store these results as `bagging_train` and `bagging_test`. Don't worry about visualizing these results yet.\n",
    "\n",
    "**2.3** _Aggregate_ all 45 _bootstrapped_ models to get a combined prediction for each training and test point: predict a 1 if and only if a majority of the models predict that example to be from class 1. What accuracy does this *bagging* model achieve on the test set? Write an assertion that verifies that this test-set accuracy is at least as good as the accuracy for the model you fit in Question 1.\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions` function (given below) to get the model's accuracy score when using only 1,2,3,4,... of the bootstrapped models. Make a plot of training and test set accuracies as a function of number of bootstraps.\n",
    "\n",
    "On your plot, also include horizontal lines for two baselines:\n",
    "- the test accuracy of the best model from question 1\n",
    "- the test accuracy of a single tree with the tree depth you chose in 2.1, trained on the full training set.\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single depth-10 tree. Explain the differences you see.\n",
    "\n",
    "**2.6** Bagging and limiting tree depth both affect how much the model overfits. Compare and contrast these two approaches. Your answer should refer to your graph in 2.4 and may duplicate something you said in your answer to 1.5.\n",
    "\n",
    "**2.7**: In what ways might our bagging classifier be overfitting the data? In what ways might it be underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints**\n",
    "- Use `resample` from sklearn to easily bootstrap the x and y data.\n",
    "- use `np.mean` to easily test for majority. If a majority of models vote 1, what does that imply about the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of `bagging_train` and `bagging_test`:**\n",
    "\n",
    "`bagging_train`:\n",
    "\n",
    "|     |bootstrap model 1's prediction|bootstrap model 2's prediction|...|bootstrap model 45's prediction|\n",
    "| --- | --- | --- | --- |\n",
    "|training row 1| binary value | binary value|... |binary value|\n",
    "|training row 2| binary value| binary value|... |binary value|\n",
    "|...| ...| ...|... |... |\n",
    "\n",
    "`bagging_test`:\n",
    "\n",
    "|     |bootstrap model 1's prediction|bootstrap model 2's prediction|...|bootstrap model 45's prediction|\n",
    "| --- | --- | --- | --- |\n",
    "|test row 1| binary value | binary value|... |binary value|\n",
    "|test row 2| binary value| binary value|... |binary value|\n",
    "|...| ...| ...|... |... |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_predictions(prediction_dataset, targets):\n",
    "    \"\"\"A function to predict examples' class via the majority among trees (ties are predicted as 0)\n",
    "    \n",
    "    Inputs:\n",
    "      prediction_dataset - a (n_examples by n_sub_models) dataset, where each entry [i,j] is sub-model j's prediction\n",
    "          for example i\n",
    "      targets - the true class labels\n",
    "    \n",
    "    Returns:\n",
    "      a vector where vec[i] is the model's accuracy when using just the first i+1 sub-models\n",
    "    \"\"\"\n",
    "    \n",
    "    n_trees = prediction_dataset.shape[1]\n",
    "    \n",
    "    # find the running percentage of models voting 1 as more models are considered\n",
    "    running_percent_1s = np.cumsum(prediction_dataset, axis=1)/np.arange(1,n_trees+1)\n",
    "    \n",
    "    # predict 1 when the running average is above 0.5\n",
    "    running_conclusions = running_percent_1s > 0.5\n",
    "    \n",
    "    # check whether the running predictions match the targets\n",
    "    running_correctnesss = running_conclusions == targets.reshape(-1,1)\n",
    "    \n",
    "    return np.mean(running_correctnesss, axis=0)\n",
    "    # returns a 1-d series of the accuracy of using the first n trees to predict the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"> <b> Question 3: Random Forests [15 pts]</b> </div>\n",
    "\n",
    "Random Forests are closely related to the bagging model we built by hand in question 2. In this question we compare our by-hand results with the results of using `RandomForestClassifier` directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**3.1** Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees that you used in Question 2.2. Evaluate its accuracy on the test set.\n",
    "\n",
    "**3.2** For each of the decision trees you fit in the bagging process, how many times is each feature used at the top node? How about for each tree in the random forest you just fit? What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs Random Forest?\n",
    "\n",
    "**Hint**: A decision tree's top feature is stored in `model.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "\n",
    "**3.3**: Make a table of the training and test accuracy for the following models:\n",
    "\n",
    "- Single tree with best depth chosen by cross-validation (from Question 1)\n",
    "- A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "- Bagging 45 such trees (from Question 2)\n",
    "- A Random Forest of 45 such trees (from Question 3.1)\n",
    "\n",
    "(This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.)\n",
    "\n",
    "What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following table (ideally in code, but ok to fill in this Markdown cell).\n",
    "\n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- | --- | --- |\n",
    "| single tree with best depth chosen by CV | | |\n",
    "| single depth-X tree | | |\n",
    "| bagging 45 depth-X trees | | |\n",
    "| Random Forest of 45 depth-X trees | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"> <b> Question 4: Boosting [15 pts]</b> </div>\n",
    "\n",
    "In this question we explore a different kind of ensemble method, boosting, where each new model is trained on a dataset weighted towards observations that the current set of models predicts incorrectly. \n",
    "\n",
    "We'll focus on the AdaBoost flavor of boosting and examine what happens to the ensemble model's accuracy as the algorithm adds more predictors to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**4.1** We'll motivate AdaBoost by noticing patterns in the errors that a single classifier makes. Fit `tree1`, a decision tree with depth 3, to the training data. For each predictor, make a plot that compares two distributions: the values of that predictor for examples that `tree1` classifies correctly, and the values of that predictor for examples that `tree1` classifies incorrectly. Do you notice any predictors for which the distributions are clearly different?\n",
    "\n",
    "**4.2** The following code attempts to implement a simplified version of boosting using just two classifiers (described below). However, it has both stylistic and functionality flaws. First, imagine that you are a grader for a Data Science class; write a comment for the student who submitted this code. Then, imagine that you're the TF writing the solutions; make an excellent example implementation. Finally, use your corrected code to compare the performance of `tree1` and the boosted algorithm on both the training and test set.\n",
    "\n",
    "**4.3** Now let's use the sklearn implementation of AdaBoost: Use `AdaBoostClassifier` to fit another ensemble to `X_train`. Use a decision tree of depth 3 as the base learner and a learning rate 0.05, and run the boosting for 800 iterations. Make a plot of the effect of the number of estimators/iterations on the model's train and test accuracy.\n",
    "\n",
    "*Hint*: The `staged_score` method provides the accuracy numbers you'll need. You'll need to use `list()` to convert the \"generator\" it returns into an ordinary list.\n",
    "\n",
    "**4.4** Repeat the plot above for a base learner with depth of (1, 2, 3, 4). What trends do you see in the training and test accuracy?\n",
    "\n",
    "(It's okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.)\n",
    "\n",
    "**4.5** Based on the plot you just made, what combination of base learner depth and number of iterations seems optimal? Why? How does the performance of this model compare with the performance of the ensembles you considered above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hints*:\n",
    "- If you have `fig, axs = plt.subplots(...)`, then `axs.ravel()` gives a list of each plot in reading order.\n",
    "- [`sns.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html) takes `ax` and `label` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intended functionality is the following:\n",
    "1. Fit `tree1`, a decision tree with max depth 3.\n",
    "2. Construct an array of sample weights. Give a weight of 1 to samples that `tree1` classified correctly, and 2 to samples that `tree1` misclassified.\n",
    "3. Fit `tree2`, another depth-3 decision tree, using those sample weights.\n",
    "4. To predict, compute the probabilities that `tree1` and `tree2` each assign to the positive class. Take the average of those two probabilities as the prediction probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boostmeup():\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1 = tree.fit(X_train, y_train)\n",
    "    sample_weight = np.ones(len(X_train))\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "      if tree1.predict([X_train[idx]]) != y_train[idx]:\n",
    "         sample_weight[idx] = sample_weight[idx] * 2\n",
    "         q = q + 1\n",
    "    print(\"tree1 accuracy:\", q / len(X_train))\n",
    "    tree2 = tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "# Train\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "        t1p = tree1.predict_proba([X_train[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_train[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_train))\n",
    "\n",
    "# Test\n",
    "    q = 0\n",
    "    for idx in range(len(X_test)):\n",
    "        t1p = tree1.predict_proba([X_test[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_test[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_test))\n",
    "\n",
    "boostmeup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"> <b> Question 5: Understanding [15 pts]</b> </div>\n",
    "\n",
    "This question is an overall test of your knowledge of this homework's material. You may need to refer to lecture notes and other material outside this homework to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "\n",
    "**5.2** Reflect on the overall performance of all of the different classifiers you have seen throughout this assignment. Which performed best? Why do you think that may have happened?\n",
    "\n",
    "**5.3** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "**5.4** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time?\n",
    "\n",
    "**5.5** Which of these techniques can be extended to regression tasks? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"> <b> Question 6: Explaining Complex Concepts Clearly [10 pts]</b> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the core skills of a data scientist is to be able to explain complex concepts clearly. To practice this skill, you'll make a short presentation of one of the approaches we have recently studied.\n",
    "\n",
    "**Choose one of the following topics:**\n",
    "\n",
    "- Decision Trees\n",
    "- Random Forests\n",
    "- Bagging\n",
    "- Boosting\n",
    "- Simple Neural Nets (like the MLP we saw in Homework 6)\n",
    "- (other topics are possible, but get staff approval first)\n",
    "\n",
    "**Make 3 slides explaining the concept.**\n",
    "\n",
    "- Focus on **clear explanations**, NOT aesthetic beauty. Photos of pen-and-paper sketches are fine if they're legible.\n",
    "- For your audience, choose **future CS109A students**.\n",
    "- You may take inspiration from anywhere, but explain in **your own words** and **make your own illustrations**.\n",
    "\n",
    "\n",
    "Submit your slides as a PDF and the source format (`.pptx`, Google Slides, etc.)\n",
    "\n",
    "NOTE: If you would be okay with us using your slides for future classes (with attribution, of course), please include a note to that effect. This will not affect your grade either way."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
